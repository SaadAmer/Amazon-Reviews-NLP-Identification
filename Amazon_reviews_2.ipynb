{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.16"
    },
    "colab": {
      "name": "lab2_Q45_SOLUTIONS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdgJ1dlMdoPQ",
        "outputId": "3658fcfa-f982-4442-b19c-709c27bbb7be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "!pip install unicodecsv\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import unicodecsv                               # csv reader\n",
        "import re                                       # regular expressions\n",
        "from sklearn.svm import LinearSVC\n",
        "from nltk.classify import SklearnClassifier\n",
        "\n",
        "# To do preprocessing\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import precision_recall_fscore_support # to report on precision and recall\n",
        "import numpy as np # To compute the average results\n",
        "\n",
        "from random import shuffle # To shuffle the dataset\n",
        "\n",
        "\n",
        "# To use feature selection in the Pipeline\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unicodecsv in /usr/local/lib/python2.7/dist-packages (0.14.1)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfzqtqXhdoPW"
      },
      "source": [
        "# load data from a file and append it to the tweetData\n",
        "def loadData(path, reviewText=None):\n",
        "    with open(path, 'rb') as f:\n",
        "        reader = unicodecsv.reader(f, encoding='utf-8', delimiter='\\t')\n",
        "        print(reader)\n",
        "        reader.next()\n",
        "        for line in reader:\n",
        "            # print(line)\n",
        "            (Id, Rating, VerifiedPurchase, Category, Text, Label) = parseReviewImproved(line)\n",
        "            rawData.append((Id, Rating, VerifiedPurchase, Category, Text, Label))\n",
        "        \n",
        "def splitData(percentage):\n",
        "    dataSamples = len(rawData)\n",
        "    halfOfData = int(len(rawData)/2)\n",
        "    trainingSamples = int((percentage*dataSamples)/2)\n",
        "    for (_, Rating, VerifiedPurchase, Category, Text, Label) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
        "        trainData.append((toFeatureVector(Rating, VerifiedPurchase, Category, preProcess(Text)),Label))\n",
        "    for (_, Rating, VerifiedPurchase, Category, Text, Label) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
        "        testData.append((toFeatureVector(Rating, VerifiedPurchase, Category, preProcess(Text)),Label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGKCnb1PdoPZ"
      },
      "source": [
        "# QUESTION 1\n",
        "\n",
        "# the output classes\n",
        "fakeLabel = 'fake'\n",
        "realLabel = 'real'\n",
        "labelMap = {'__label1__' : fakeLabel, '__label2__' : realLabel}\n",
        "\n",
        "# Convert line from input file into an id/text/label tuple plus meta features\n",
        "def parseReviewImproved(reviewLine):\n",
        "    Id    = int(reviewLine[0])\n",
        "    Rating = int(reviewLine[2])\n",
        "    VerifiedPurchase = reviewLine[3]\n",
        "    Category = reviewLine[4]\n",
        "    Text  = reviewLine[8]\n",
        "    Label = labelMap[reviewLine[1]]\n",
        "    return (Id, Rating, VerifiedPurchase, Category, Text, Label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-j__fZvdoPd",
        "outputId": "51f505fa-df7f-4691-dce5-9e9271378f4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# TEXT PREPROCESSING AND FEATURE VECTORIZATION\n",
        "\n",
        "# input: a string of one review\n",
        "def preProcess(text):\n",
        "    # should return a list of tokens\n",
        "    \n",
        "    # word tokenisation, including punctuation removal\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    \n",
        "    # lowercasing\n",
        "    tokens = [t.lower() for t in tokens]\n",
        "\n",
        "    # stopword removal- benefits are it removes rare words, though bad for bigram relations\n",
        "    if False:\n",
        "        stop = set(stopwords.words('english'))\n",
        "        tokens = [t for t in tokens if t not in stop]\n",
        "    \n",
        "    # lemmatisation\n",
        "    lemmatiser = WordNetLemmatizer()\n",
        "    tokens = [lemmatiser.lemmatize(t) for t in tokens]\n",
        "    tokens = [t for t in tokens if t] # ensure no empty space\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "print(preProcess(\"hello this is the, ehh... presumably, a crying situations!\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hello', 'this', 'is', 'the', 'ehh', 'presumably', 'a', u'cry', u'situation']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xmTp_9GdoPg",
        "outputId": "1e873055-7b56-47dd-9945-3eaef1fb5642",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# QUESTION 2\n",
        "featureDict = {} # the global feature dictionary\n",
        "\n",
        "def toFeatureVector(rating, verifiedPurchase, category, tokens):\n",
        "    # return a dictionary 'featureVect' where the keys are the tokens in 'words' and the values are the number of occurrences of the tokens\n",
        "    # start by using binary values only:\n",
        "#     baseDict = {}\n",
        "    # print(tokens)\n",
        "    featureVec = {}\n",
        "\n",
        "    for w in tokens:\n",
        "        try:\n",
        "            featureVec[w] += 1.0/len(tokens)\n",
        "        except KeyError:\n",
        "            featureVec[w] = 1.0/len(tokens)\n",
        "        try:\n",
        "            featureDict[w] += 1.0/len(tokens)\n",
        "        except KeyError:\n",
        "            featureDict[w] = 1.0/len(tokens)\n",
        "    \n",
        "    # just get bigram binary presence or not\n",
        "    for i in range(1, len(tokens)):\n",
        "        bigram = tokens[i-1] + \" \" + tokens[i]\n",
        "        try:\n",
        "            featureVec[bigram] = 1 #+= 1.0/len(tokens)\n",
        "        except KeyError:\n",
        "            featureVec[bigram] = 1 #= 1.0/len(tokens)\n",
        "        try:\n",
        "            featureDict[bigram] += 1.0\n",
        "        except KeyError:\n",
        "            featureDict[bigram] = 1.0\n",
        "\n",
        "    featureVec['RATING:'+str(rating)] = 1.0 #0.3\n",
        "    featureVec['VP:'+verifiedPurchase] = 1.0 # 0.5\n",
        "    featureVec['CATEGORY:'+category] = 1.0 #0.3\n",
        "    \n",
        "    try:\n",
        "        featureDict['RATING:'+str(rating)] += 1.0\n",
        "    except KeyError:\n",
        "        featureDict['RATING:'+str(rating)] = 1.0\n",
        "        \n",
        "    try:\n",
        "        featureDict['VP:'+verifiedPurchase] += 1.0\n",
        "    except KeyError:\n",
        "        featureDict['VP:'+verifiedPurchase] = 1.0\n",
        "        \n",
        "    try:\n",
        "        featureVec['CATEGORY:'+category] += 1.0\n",
        "    except KeyError:\n",
        "         featureDict['CATEGORY:'+category] = 1.0\n",
        "    \n",
        "    return featureVec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrsqyvVxdoPi"
      },
      "source": [
        "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
        "def trainClassifier(trainData):\n",
        "    print \"Training Classifier...\"\n",
        "    pipeline =  Pipeline([('tfidf', TfidfTransformer()),('chi2', SelectKBest(chi2, k=20000)),('svc', LinearSVC(loss = 'hinge'))])\n",
        "    return SklearnClassifier(pipeline).train(trainData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTrdL3CedoPl"
      },
      "source": [
        "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
        "\n",
        "def predictLabels(reviewSamples, classifier):\n",
        "    return classifier.classify_many(map(lambda t: t[0], reviewSamples))\n",
        "\n",
        "def predictLabel(text, classifier):\n",
        "    return classifier.classify(toFeatureVector(preProcess(text)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAY0iE8HdoPn"
      },
      "source": [
        "# QUESTION 3\n",
        "\n",
        "def crossValidate(dataset, folds):\n",
        "    shuffle(dataset)\n",
        "    results = []\n",
        "    foldSize = len(dataset)/folds\n",
        "    \n",
        "    for i in range(0,len(dataset),foldSize):\n",
        "        # insert code here that trains and tests on the 10 folds of data in the dataset\n",
        "        print \"Fold start on items %d - %d\" % (i, i+foldSize)\n",
        "        myTestData = dataset[i:i+foldSize]\n",
        "        myTrainData = dataset[:i] + dataset[i+foldSize:]\n",
        "        classifier = trainClassifier(myTrainData)\n",
        "        y_true = map(lambda x: x[1], myTestData)\n",
        "        y_pred = predictLabels(myTestData, classifier)\n",
        "        results.append(precision_recall_fscore_support(y_true, y_pred, average='weighted'))\n",
        "        \n",
        "    avgResults = map(np.mean,zip(*results)[:3])\n",
        "    return avgResults"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APfehT3gdoPq",
        "outputId": "dae61944-6bc3-4bf4-efbf-f7c65864ed11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "# MAIN\n",
        "\n",
        "# loading reviews\n",
        "rawData = [] # the filtered data from the dataset file (should be 21000 samples)\n",
        "trainData = [] # the training data as a percentage of the total dataset (currently 80%, or 16800 samples)\n",
        "testData = [] # the test data as a percentage of the total dataset (currently 20%, or 4200 samples)\n",
        "\n",
        "# references to the data files\n",
        "reviewPath = '/amazon_reviews.txt'\n",
        "\n",
        "# do the actual stuff\n",
        "print \"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData))\n",
        "print \"Preparing the dataset...\"\n",
        "loadData(reviewPath)\n",
        "print \"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData))\n",
        "print \"Preparing training and test data...\"\n",
        "splitData(0.8)\n",
        "print \"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData))\n",
        "\n",
        "\n",
        "# print(trainData)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now 0 rawData, 0 trainData, 0 testData\n",
            "Preparing the dataset...\n",
            "<unicodecsv.py2.UnicodeReader object at 0x7fec6b1ed290>\n",
            "Now 21000 rawData, 0 trainData, 0 testData\n",
            "Preparing training and test data...\n",
            "Now 21000 rawData, 16800 trainData, 4200 testData\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gVdkDRidoPu",
        "outputId": "bd5c5a9c-a391-4b6e-c5ac-9cb197c2a378",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        }
      },
      "source": [
        "cv_results = crossValidate(trainData, 10)\n",
        "print cv_results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold start on items 0 - 1680\n",
            "Training Classifier...\n",
            "Fold start on items 1680 - 3360\n",
            "Training Classifier...\n",
            "Fold start on items 3360 - 5040\n",
            "Training Classifier...\n",
            "Fold start on items 5040 - 6720\n",
            "Training Classifier...\n",
            "Fold start on items 6720 - 8400\n",
            "Training Classifier...\n",
            "Fold start on items 8400 - 10080\n",
            "Training Classifier...\n",
            "Fold start on items 10080 - 11760\n",
            "Training Classifier...\n",
            "Fold start on items 11760 - 13440\n",
            "Training Classifier...\n",
            "Fold start on items 13440 - 15120\n",
            "Training Classifier...\n",
            "Fold start on items 15120 - 16800\n",
            "Training Classifier...\n",
            "[0.81293728234612, 0.8069047619047618, 0.8059974876323489]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HC0TIGCdoP3",
        "outputId": "ece69546-7429-4eb8-dc5b-a16b4f308a7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "classifier = trainClassifier(trainData)\n",
        "testTrue = map(lambda t: t[1], testData)\n",
        "testPred = predictLabels(testData, classifier)\n",
        "finalScores = precision_recall_fscore_support(testTrue, testPred, average='weighted')\n",
        "print \"Done training!\"\n",
        "print \"Precision: %f\\nRecall: %f\\nF Score:%f\" % finalScores[:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Classifier...\n",
            "Done training!\n",
            "Precision: 0.818834\n",
            "Recall: 0.810476\n",
            "F Score:0.809226\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7xetI1MdoP7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg6EqkUUdoQA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}